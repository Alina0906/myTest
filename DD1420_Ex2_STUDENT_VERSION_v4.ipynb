{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alina0906/myTest/blob/alina/DD1420_Ex2_STUDENT_VERSION_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecce179f"
      },
      "outputs": [],
      "source": [
        "#! Required python packages: numpy, scipy, scikit-learn, matplotlib, seaborn\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "id": "ecce179f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9gy3oaCdXtf"
      },
      "source": [
        "**Fill in your name and your partner's name below** (and name the `.ipynb` file correctly):\n",
        "\n",
        "---\n",
        "\n",
        "### YOUR NAME\n",
        "\n",
        "### YOUR PARTNER'S NAME\n",
        "\n",
        "---"
      ],
      "id": "Z9gy3oaCdXtf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlP47NhhLUFw"
      },
      "source": [
        "# Exercise 2 - Machine Learning & Optimization (DD1420)\n",
        "\n",
        "## Instructions\n",
        "\n",
        "This Jupyter Notebook contains exercises for DD1420 **Module 2: Machine Learning & Optimization**. To receive credit, all cells must be completed and must function correctly.\n",
        "\n",
        "### Collaboration\n",
        "\n",
        "- **Collaboration**: You are allowed to work on this exercise in pairs. However, each student must submit their own completed notebook. If you worked with a partner, please indicate your partner in the filename as described below.\n",
        "- **Code of Conduct**: Adhere to the following:\n",
        "  1. All group members are responsible for the work submitted.\n",
        "  2. Each student must honestly disclose any assistance received or external sources used.\n",
        "  3. Do not copy from other students' solutions. If you need help, use the appropriate Discussion Topic or sign up for a help session.\n",
        "\n",
        "### Completing the Exercise\n",
        "\n",
        "- **Code Cells**: Replace any placeholder comments like `YOUR CODE HERE` or \"YOUR ANSWER HERE\" with your own code or response. Once implemented, delete any instances of `raise NotImplementedError()`.\n",
        "- **Library Imports**: Do not import any additional libraries beyond those already included in the assignment.\n",
        "- **Derivations**: For derivation questions, you may use $\\LaTeX$ in markdown cells or upload an image of your handwritten derivation. In *Google Colab*, to upload an image, create a text cell, click the `Insert Image` icon, and upload your file.\n",
        "\n",
        "### Submission Instructions\n",
        "\n",
        "1. **Final Check**: Before submitting, ensure everything runs correctly by selecting `Runtime` -> `Restart and Run All`.\n",
        "2. **File Naming**: Download the notebook by selecting `File` -> `Download` -> `Download .ipynb`. Rename the file as follows:\n",
        "  - `Ex??_YOURLASTNAME_YOURFIRSTNAME_and_PARTNERLASTNAME_PARTNERFIRSTNAME.ipynb`\n",
        "  - Replace `??` with the correct exercise number. If you worked alone, omit the partner's name from the filename.\n",
        "3. **Submission**: Submit the `.ipynb` file to Canvas.\n",
        "\n",
        "### Oral Examination\n",
        "\n",
        "- **Preparation**: During the oral examination, you will be asked to demonstrate and explain your work. Have the link to your `.ipynb` file ready, with all cells executed.\n",
        "\n"
      ],
      "id": "hlP47NhhLUFw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adefc2aa"
      },
      "source": [
        "## 2.1 Linear Regression with RANSAC\n"
      ],
      "id": "adefc2aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-k1Jvv4iMf_"
      },
      "source": [
        "A mad scientist has created an artificial dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$ according to the equation:\n",
        "\n",
        "$$\n",
        "y = e \\cdot x + e + \\varepsilon\n",
        "$$\n",
        "\n",
        "where $e$ is the Euler constant, and $\\varepsilon \\in U(-1, 1)$, meaning $\\varepsilon$ is a random error sampled from a uniform distribution over the interval $(-1, +1)$.\n",
        "\n",
        "Our objective is to reverse-engineer the parameters $w = e$ and $b = e$ by performing linear regression on this dataset. The dataset $D$ is generated using the following Python code:\n"
      ],
      "id": "5-k1Jvv4iMf_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3d211ef"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "N = 100\n",
        "xs = np.linspace(0, 10, N)\n",
        "ys = np.e * xs + np.e + (2 * np.random.rand(N) - 1)"
      ],
      "id": "c3d211ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b052174"
      },
      "source": [
        "Here, np.random.rand generates uniform random samples within the unit interval $(0, 1)$. We transform these samples into the desired interval $(-1, 1)$ by multiplying each sample by 2 and subtracting 1.\n",
        "\n",
        "You can visualize the generated dataset by plotting the points:"
      ],
      "id": "7b052174"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1597269e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(xs, ys)"
      ],
      "id": "1597269e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81d9190"
      },
      "source": [
        "**Applying the RANSAC Algorithm**\n",
        "\n",
        "We will apply a simplified version of the RANSAC (Random Sample Consensus) algorithm to identify the parameters $w$ and $b$. The algorithm proceeds in the following steps:\n",
        "\n",
        "1. **Randomly select two points** from the dataset.\n",
        "2. **Find the line** that passes through these two points. This line will have a slope $w$ and an intercept $b$.\n",
        "3. **Evaluate the line** by checking how well it fits the remaining points in the dataset. Specifically, we measure how many points lie within a certain distance (inlier threshold) from the line.\n",
        "\n",
        "**Key Considerations**\n",
        "\n",
        "- **Iteration**: RANSAC typically iterates over many random pairs of points, repeatedly applying the above steps. The line that best fits the majority of the points (i.e., has the most inliers) is selected as the final model.\n",
        "\n",
        "- **Parameter Estimation**: Once the best line is found, the final parameters $w$ and $b$ are estimated from the inliers.\n"
      ],
      "id": "c81d9190"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d9a69f3"
      },
      "source": [
        "**2.1.1.** *Write a function ```size_two_subsets = all_size_two_subsets(ls)``` that accepts a list ```ls``` as input and outputs a list ```size_two_subsets``` of all subsets of size 2.*"
      ],
      "id": "8d9a69f3"
    },
    {
      "cell_type": "code",
      "source": [
        "def all_size_two_subsets(ls):\n",
        "  \"\"\"Returns a list of all subsets of size two.\"\"\"\n",
        "    # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "f1sU0pdg4brq"
      },
      "id": "f1sU0pdg4brq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69a31c0d"
      },
      "source": [
        "**2.1.2. Fitting a Line**\n",
        "\n",
        "*Write a function `w, b = linear_fit(x1, y1, x2, y2)` that accepts two points \\( p_1 = (x_1, y_1) \\) and \\( p_2 = (x_2, y_2) \\), and outputs the parameters \\( w \\) and \\( b \\) of the line defined by \\( w \\cdot x + b \\) that passes through these two points.*\n",
        "\n",
        "*The function should calculate the slope \\( w \\) as the change in \\( y \\) divided by the change in \\( x \\), and then determine the intercept \\( b \\) using one of the points. This will form the linear equation that best fits the two points.*\n"
      ],
      "id": "69a31c0d"
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fit(x1, y1, x2, y2):\n",
        "  \"\"\"Returns w,b that fit (x1,y1) and (x2,y2)\"\"\"\n",
        "  # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "yAx4vGPf41hw"
      },
      "id": "yAx4vGPf41hw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW98MiUTq6g4"
      },
      "source": [
        "**2.1.3. Scoring the Fit**\n",
        "\n",
        "*Write a function `score_candidate(w, b, xs, ys)` that computes how well the linear equation \\( w \\cdot x + b \\) fits the entire dataset. To do this, calculate the difference between each actual \\( y_i \\) value and the predicted value from the equation, \\( w \\cdot x_i + b \\).*\n",
        "\n",
        "*Your function should compute the Root Mean Square Error (RMSE), which is a common metric for measuring the average magnitude of the errors between predicted and actual values. RMSE gives you a sense of how well your line approximates the data points:*\n",
        "- **Lower RMSE values** indicate a better fit.\n",
        "- **Higher RMSE values** suggest a poor fit, potentially due to outliers or an incorrect model.\n"
      ],
      "id": "IW98MiUTq6g4"
    },
    {
      "cell_type": "code",
      "source": [
        "def score_candidate(w, b, xs, ys):\n",
        "  \"\"\"Returns the RMSE for a given w,b\"\"\"\n",
        "  # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "pa6Ejft945F5"
      },
      "id": "pa6Ejft945F5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "_360dNTHpmT3",
      "metadata": {
        "id": "_360dNTHpmT3"
      },
      "source": [
        "How good of a fit we get if we use the first two points in the list?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9egblqYuptA-",
      "metadata": {
        "id": "9egblqYuptA-"
      },
      "outputs": [],
      "source": [
        "x1 = xs[0]\n",
        "x2 = xs[1]\n",
        "y1 = ys[0]\n",
        "y2 = ys[1]\n",
        "w,b = linear_fit(x1,y1,x2,y2)\n",
        "print('RMSE =',score_candidate(w,b,xs,ys))\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xs, w * xs + b, 'y-', linewidth=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It57MMSDrCKT"
      },
      "source": [
        "**2.1.4. Applying Simplified RANSAC**\n",
        "\n",
        "*Now, implement a simplified RANSAC algorithm by evaluating `score_candidate` for every possible two-element subset from `size_two_subsets`. Your goal is to find the best-fitting parameters $w^\\star$ and $b^\\star$ by minimizing the RMSE across all possible pairs.*\n",
        "\n",
        "*For each subset, calculate $w$ and $b$ using your `linear_fit` function, and then determine the RMSE using `score_candidate`. Track the best $w$ and $b$ values, and print them each time the RMSE improves, along with the current RMSE score.*\n",
        "\n",
        "This iterative process simulates how RANSAC refines the fit by continuously searching for the best line that represents the dataset.\n"
      ],
      "id": "It57MMSDrCKT"
    },
    {
      "cell_type": "code",
      "source": [
        "w_star, b_star = None, None\n",
        "\n",
        "pts = np.stack([xs, ys], axis=1)\n",
        "size_two_subsets = all_size_two_subsets(pts)\n",
        "w_star, b_star = None, None\n",
        "best_score = np.inf\n",
        "for size_two_subset in size_two_subsets:\n",
        "  # YOUR CODE HERE\n",
        "  if current_score < best_score:\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "# plotting your solution\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xs, w_star * xs + b_star, 'y-', linewidth=3)"
      ],
      "metadata": {
        "id": "PZ3ZWgUt5BCn"
      },
      "id": "PZ3ZWgUt5BCn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7649a937",
      "metadata": {
        "id": "7649a937"
      },
      "source": [
        "## 2.2 Applying the algebraic solution to linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a356cb44",
      "metadata": {
        "id": "a356cb44"
      },
      "source": [
        "Now that you have an intuition of how linear regression works through RANSAC and basic point selection, let's solidify your understanding by applying some basic linear algebra. In this exercise, you will use an algebraic approach to linear regression. Below is a small dataset generated by the mad scientist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eJg13IS6_cNo",
      "metadata": {
        "id": "eJg13IS6_cNo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "n = 20\n",
        "xs = np.linspace(0, 1, n)\n",
        "ys = np.e * xs + np.pi + np.random.randn(n)\n",
        "plt.scatter(xs, ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpi_lQ09sC-1"
      },
      "source": [
        "**2.2.1** *Show that prepending $1$s to $x$ and having a single matrix multiplication using $\\theta^T$ is equivalent to the linear equation $ y = w^T x + b$.*\n",
        "\n",
        "*hint: You can explicitly do the multiplication after defining the new terms, e.g.,*\n",
        "\n",
        "$ \\theta^T =\n",
        " \\begin{bmatrix}\n",
        "  b_{1} & w_{1,1} & \\cdots & w_{1,n}\\\\\n",
        "  b_{2} & w_{2,1} & \\cdots & w_{2,n} \\\\\n",
        "  \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
        "  b_{m} & w_{m,1} & \\cdots & w_{m,n}\n",
        " \\end{bmatrix} = \\begin{bmatrix}\n",
        "  b_{1} & 0 & \\cdots & 0\\\\\n",
        "  b_{2} & 0 & \\cdots & 0 \\\\\n",
        "  \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
        "  b_{m} & 0 & \\cdots & 0\n",
        " \\end{bmatrix} + \\begin{bmatrix}\n",
        "  0 & w_{1,1} & \\cdots & w_{1,n}\\\\\n",
        "  0 & w_{2,1} & \\cdots & w_{2,n} \\\\\n",
        "  \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
        "  0 & w_{m,1} & \\cdots & w_{m,n}\n",
        " \\end{bmatrix}$\n"
      ],
      "id": "lpi_lQ09sC-1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$\n",
        "\n"
      ],
      "metadata": {
        "id": "unAgSCbWsSK9"
      },
      "id": "unAgSCbWsSK9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.2** *Write a function ```b, w = linear_regression(x, y)``` that performs linear regression using the algebraic solution. Recall that the function that we want to minimize is*\n",
        "\n",
        "$L = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$\n",
        "\n",
        "\n",
        " *Use the matrix solution we found in the lecture notes for $\\theta^*$. Apply it to the dataset above. Plot the solution and print the values.*"
      ],
      "metadata": {
        "id": "O3z_h0TjLnX9"
      },
      "id": "O3z_h0TjLnX9"
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression(xs, ys):\n",
        "    \"\"\"\n",
        "    Returns an algebraic solution for w,b given\n",
        "    xs: np.ndarray of shape [n]\n",
        "    ys: np.ndarray of shape [n]\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # X = ... should be of shape (20,2), remember to prepend 1 to each point\n",
        "    # X would look something like this\n",
        "    # [ 1   x_1\n",
        "    #   1   x_2\n",
        "    #   ...\n",
        "    #   1   x_n ]\n",
        "    # Y = ... should be of shape (20,1)\n",
        "    # implement the theta* equation in the matrix form we derived at the end of the lecture notes 2.2\n",
        "\n",
        "    return theta\n",
        "b, w = linear_regression(xs,ys)\n",
        "print('w = ', w, 'b = ', b)\n",
        "zs = xs*w + b\n",
        "_, ax = plt.subplots()\n",
        "ax.scatter(xs, ys)\n",
        "ax.plot(xs, zs, 'y-', linewidth=3);\n",
        "ax.legend(['Fitted line']);"
      ],
      "metadata": {
        "id": "iRaZZR4sTVQ7"
      },
      "id": "iRaZZR4sTVQ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "978ebded",
      "metadata": {
        "id": "978ebded"
      },
      "source": [
        "## 2.3 Using linear regression for non-linear relationships\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aykbMiG82xWQ",
      "metadata": {
        "id": "aykbMiG82xWQ"
      },
      "source": [
        "Linear regression is often misunderstood in the sense that people believe that it can only be used to fit \"lines\" (i.e. for *linear* relationships between $x$ and $y$) even though the \"linearity\" refers to being linear with respect to the weights (i.e. the relationship between $w$ and $y$ is assumed to be linear).\n",
        "\n",
        "Take a look at graph of the dataset $\\mathcal{D} = \\lbrace (x_i, y_i) \\rbrace$ showing a non-linear relationships between $x$ and $y$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbfb1155",
      "metadata": {
        "id": "bbfb1155"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 20\n",
        "np.random.seed(0)\n",
        "xs = 2 - 3 * np.random.normal(0, 1, n)\n",
        "ys = xs - 2 * xs ** 2 + 0.5 * xs ** 3 + np.random.normal(-3, 3, n)\n",
        "plt.scatter(xs,ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b45d845",
      "metadata": {
        "id": "2b45d845"
      },
      "source": [
        "If we naively apply **SK-learn**'s linear regression function, we use the model\n",
        "\n",
        "$y = \\theta^{\\intercal}x = w^T x + b$\n",
        "\n",
        "and we just end up fitting a straight line though the points. 😞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c6f93a",
      "metadata": {
        "id": "c8c6f93a"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# here is how you define and train a model with sklearn\n",
        "model = LinearRegression()\n",
        "model.fit(xs[:, np.newaxis], ys[:, np.newaxis])\n",
        "\n",
        "# here is how you make predictions\n",
        "y_pred = model.predict(xs[:, np.newaxis])\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(ys,y_pred))\n",
        "print('RMSE = ', rmse)\n",
        "\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xs, y_pred, 'y-', linewidth=3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "337c74eb",
      "metadata": {
        "id": "337c74eb"
      },
      "source": [
        "To achieve a better fit for this non-linear relationship, we can use a cubic polynomial. This means transforming our original input \\( x \\) into polynomial features. Define $ \\Phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ x^3 \\end{bmatrix} $, then the model equation is:\n",
        "\n",
        "\n",
        "$y = \\theta^T \\Phi(x) = b + w_1x + w_2x^2 + w_3x^3$\n",
        "\n",
        "\n",
        "Notice that while the model now captures non-linear relationships between $ x $ and $ y $, the relationship between the parameters $ \\theta $ and the output $ y $ remains linear. This is the key idea behind using linear regression with polynomial features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9n9YNQCslOR"
      },
      "source": [
        "**2.3.1.** *Write a function ```x_poly = polynomial_features(x)```** that computes $\\Phi(x)$ for each $x$, transforming it to ``x_poly``.*"
      ],
      "id": "g9n9YNQCslOR"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "def polynomial_features(xs):\n",
        "    \"\"\"\n",
        "    x: A np.ndarray of shape [N]\n",
        "    Returns:\n",
        "        x_poly: A np.ndarray of shape [N, 4]\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return x_poly"
      ],
      "metadata": {
        "id": "9wF6ffwSJIEA"
      },
      "id": "9wF6ffwSJIEA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can do the same thing, with less coding, using the sklearn library as well. Try that here and double check you are getting the same results.\n",
        "\n",
        "*hint: [look up](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) the function we imported.*"
      ],
      "metadata": {
        "id": "MquajQdFnnlI"
      },
      "id": "MquajQdFnnlI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b254b65"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "def polynomial_features_sklearn(x):\n",
        "    \"\"\"\n",
        "    x: A np.ndarray of shape [N]\n",
        "    Returns:\n",
        "        x_poly: A np.ndarray of shape [N, 4]\n",
        "    \"\"\"\n",
        "    polynomial_features= PolynomialFeatures(degree=3)\n",
        "    x_poly = polynomial_features.fit_transform(x[:, np.newaxis])\n",
        "    return x_poly"
      ],
      "id": "5b254b65"
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.allclose(polynomial_features_sklearn(xs), polynomial_features(xs))"
      ],
      "metadata": {
        "id": "36xnGrsCm-C2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "36xnGrsCm-C2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal here will be to get the model to predict the outputs given the (polynomial) input features. Here is how you would get the predictions (not the coefficients) in sklearn:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(x, y)\n",
        "predictions = model.predict(x)\n",
        "```\n",
        "\n",
        "\n",
        "There is an obvious question here: Why would we try and predict *y* values for points we already know the *y*? The answer is that we usually would not, at least not in ML. This exercise is just to get your feet wet."
      ],
      "metadata": {
        "id": "iuqMMgH2i4_9"
      },
      "id": "iuqMMgH2i4_9"
    },
    {
      "cell_type": "markdown",
      "id": "8d280686",
      "metadata": {
        "id": "8d280686"
      },
      "source": [
        "**2.3.2** *Write a function `w = polynomial_linear_regression(x_poly, y)` that performs linear regression on the transformed polynomial features `x_poly` and outputs the optimal parameter vector $ \\theta^* \\in \\mathbb{R}^4 $. This parameter vector represents the best-fit weights $ \\theta $ for the cubic polynomial model.*\n",
        "\n",
        "*Hint*: Use the `LinearRegression` model from sklearn, and ensure you retrieve the coefficients after fitting the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def polynomial_linear_regression(x_poly, ys):\n",
        "    \"\"\"\n",
        "    x: A np.ndarray of shape [N, 4]\n",
        "    y: A np.ndarray of shape [N]\n",
        "    Returns:\n",
        "        w: A np.ndarray of shape [4]\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return model.coef_"
      ],
      "metadata": {
        "id": "ShxKhczgJOH0"
      },
      "id": "ShxKhczgJOH0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bf500f46",
      "metadata": {
        "id": "bf500f46"
      },
      "source": [
        "**2.3.3** *Write a function* ```y_poly_pred = apply_weights(x_poly, w)``` *that applies the weights to* $x$, $\\theta^{\\intercal}\\phi(x)$ *to get a prediction.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_weights(x_poly, w):\n",
        "    \"\"\"\n",
        "    x_poly: A np.ndarray of shape [N, 4]\n",
        "    w: A np.ndarray of shape [4]\n",
        "    Returns:\n",
        "      y_pred: the predicted value for x\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # return ... # do the matrix multiplication, pay attention to the dimensions"
      ],
      "metadata": {
        "id": "svNYDr62JVe9"
      },
      "id": "svNYDr62JVe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34ec6095",
      "metadata": {
        "id": "34ec6095"
      },
      "source": [
        "**2.3.4.** Fit the model to the transformed polynomial data to find $ \\theta^* $. After fitting the model, compute the RMSE (Root Mean Square Error) to evaluate how well your model's predictions match the actual data.\n",
        "\n",
        "*Hint*: Use the `mean_squared_error` function from sklearn to calculate RMSE conveniently by taking the square root of the mean squared error value.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "# YOUR CODE HERE\n",
        "# x_poly = ... # get the polynomial features\n",
        "# w = ... # find the fitted weights\n",
        "# y_poly_pred = ... # do the predictions\n",
        "# rmse = ...\n",
        "print('RMSE = ', rmse)"
      ],
      "metadata": {
        "id": "btpDvhhJKJIJ"
      },
      "id": "btpDvhhJKJIJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "47db12b1",
      "metadata": {
        "id": "47db12b1"
      },
      "source": [
        "Finally, let's visualize the polynomial fit by plotting the predicted curve against the original data points. This will allow us to see how well the cubic polynomial model captures the non-linear relationship between $ x $ and $ y $.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9233bdce",
      "metadata": {
        "id": "9233bdce"
      },
      "outputs": [],
      "source": [
        "xi = np.arange(min(xs),max(xs),0.1)\n",
        "xi_poly = polynomial_features(xi)\n",
        "yi_poly_pred = apply_weights(xi_poly,w)\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xi, yi_poly_pred,'y-')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "564dccf2",
      "metadata": {
        "id": "564dccf2"
      },
      "source": [
        "## 2.4 Logistic regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e3cd50",
      "metadata": {
        "id": "12e3cd50"
      },
      "source": [
        "In this exercise, we will explore logistic regression, a powerful technique for binary classification. The mad scientist has provided us with another dataset, but this time it consists of two distinct classes, represented by yellow and blue samples.\n",
        "\n",
        "Our goal is to use logistic regression to classify these samples into their respective classes and understand the underlying theory and implementation of the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec033de",
      "metadata": {
        "id": "dec033de"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "xs, ys = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, class_sep=.65)\n",
        "plt.figure(0)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9_Hsr03vl3u",
      "metadata": {
        "id": "q9_Hsr03vl3u"
      },
      "source": [
        "This dataset presents a clear binary classification problem, where our task is to assign each sample to one of two classes. Logistic regression is particularly well-suited for this type of problem because it models the probability that a given sample belongs to a particular class using a sigmoid function.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "As we saw in the Lecture Notes, *logistic regression* is motivated by the desire to classify a sample (e.g. some vector) into two classes $\\pmb{y} = \\{0,1\\}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\mathcal{P}_{\\pmb{y}|\\pmb{x}} (y| \\pmb{x} = x_i) = p^{\\pmb{y}} (1-p)^{1-\\pmb{y}}\\quad $   \n",
        "\n",
        "<br>\n",
        "\n",
        "where $\\quad \\hat{p} = \\sigma(\\theta^{\\intercal} x_i) $ and $\\sigma(x) = \\frac{1}{1+e^{-x}}$ is the so-called *sigmoid* (or logistic) function. Plugging in gives\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\mathcal{P}_{\\pmb{y}|\\pmb{x}} (y| \\pmb{x} = x_i) = \\sigma(\\theta^{\\intercal} x_i)^{y_i} \\, (1 - \\sigma(\\theta^{\\intercal} x_i))^{1-y_i}$\n",
        "\n",
        "<br>\n",
        "\n",
        "and taking the $\\log$ gives the *binary cross entropy* loss\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "\\log \\mathcal{P}_{\\pmb{y}|\\pmb{x}} (y| \\pmb{x} = x_i)\n",
        "&= y_i \\log \\sigma(\\theta^{\\intercal} x_i) - (1 - y_i) \\log (1-\\sigma(\\theta^{\\intercal} x_i)) \\\\\n",
        "&= y_i \\log \\sigma(w^{\\intercal} x_i + b) - (1 - y_i) \\log (1-\\sigma(w^{\\intercal} x_i + b))\n",
        "\\end{align}$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "We can turn this into a total loss function $L$ by summing over the dataset and adding a minus ($-$) sign because we want to minimize it\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "L(w,b) &= -\\sum_{i=1}^n \\lbrace y_i \\cdot \\log \\sigma(w^T \\cdot x_i + b) + (1-y_i) \\cdot \\log (1 - \\sigma(w^T \\cdot x_i + b))  \\rbrace\n",
        "\\end{align}$\n",
        "\n",
        "This loss function measures how well the logistic regression model predicts the true class labels across the entire dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M61KhMWjAACa",
      "metadata": {
        "id": "M61KhMWjAACa"
      },
      "source": [
        "**2.4.1** *To optimize the logistic regression model, we need to compute the gradients of the binary cross-entropy loss with respect to the model parameters $ w $ and $ b $. These gradients will guide the update of the model parameters during training.*\n",
        "\n",
        "*In this task, you will derive the gradient of the loss function with respect to $ w $. This is a crucial step in understanding how logistic regression learns from data. Fill in the missing steps to show that the derivative of the total binary cross-entropy loss $ L $ with respect to $ w $ is given by...*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zbb01sIdAz_k",
      "metadata": {
        "id": "Zbb01sIdAz_k"
      },
      "source": [
        "$\\begin{align}\n",
        "\\frac{\\partial}{\\partial w}\\, L(w,b) &= \\frac{\\partial}{\\partial w}\\, -\\sum_{i=1}^n \\lbrace y_i \\cdot \\log \\sigma(w^T \\cdot x_i + b) + (1-y_i) \\cdot \\log (1 - \\sigma(w^T \\cdot x_i + b))  \\rbrace\n",
        "\\end{align}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "\\frac{\\partial}{\\partial w}\\, L(w,b) &= \\sum_{i=1}^n \\lbrace (1-y_i) \\cdot x_i \\cdot \\sigma(w^T x_i + b) - y_i \\cdot x_i \\cdot \\sigma(-w^T x_i - b) \\rbrace\n",
        "\\end{align}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-GZYmBzRBafW",
      "metadata": {
        "id": "-GZYmBzRBafW"
      },
      "source": [
        "Hint: a couple of identities can help simplify the equations:\n",
        "\n",
        "\\begin{align}\n",
        "1 - \\sigma(x) &= \\sigma(-x) \\\\\n",
        "\\frac{\\partial}{\\partial \\mathbf{w}} \\log \\sigma(g(\\mathbf{w})) &= \\frac{\\partial g}{\\partial \\mathbf{w}} \\cdot \\sigma(-g(\\mathbf{w}))\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i0qPhJmsG0f8",
      "metadata": {
        "id": "i0qPhJmsG0f8"
      },
      "source": [
        "**2.4.2** *Show that the derivative of the binary cross entropy loss* $L$ *w.r.t. $b$ is given by (fill in the missing steps)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4UgfCFD1G6Fx",
      "metadata": {
        "id": "4UgfCFD1G6Fx"
      },
      "source": [
        "$\\begin{align}\n",
        "\\frac{\\partial}{\\partial b}\\, L(w,b) &= \\frac{\\partial}{\\partial b}\\, -\\sum_{i=1}^n \\lbrace y_i \\cdot \\log \\sigma(w^T x_i + b) + (1-y_i) \\cdot \\log (1 - \\sigma(w^T x_i + b))  \\rbrace\n",
        "\\end{align}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "\\frac{\\partial}{\\partial b}\\, L(w,b) &= \\sum_{i=1}^n \\lbrace (1-y_i) \\cdot \\sigma(w^T x_i + b) - y_i \\cdot \\sigma(-w^T x_i - b) \\rbrace\n",
        "\\end{align}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "djzoZnINLepR",
      "metadata": {
        "id": "djzoZnINLepR"
      },
      "source": [
        "With these gradients in hand, your task is now to implement *batch stochastic gradient descent (SGD)*. Instead of looping over all samples $n$ you will loop over $B$ samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00923f3",
      "metadata": {
        "id": "d00923f3"
      },
      "source": [
        "**2.4.3**  *Write a function `xs_batch, ys_batch = sample_batch(xs, ys, B)` that randomly samples $ B $ elements from both `xs` and `ys`. This function should return two arrays: `xs_batch` containing the sampled input features, and `ys_batch` containing the corresponding labels.*\n",
        "\n",
        "FYI, consider for a moment how we would write the sampling operation in mathematical notation. Most common notation would be $(x, y) ∼ P(x,y)$, i.e. sampling from a joint distribution. But as is common, we only have a training set, not the actual distribution. How would we denote it in this case? Usually, it is denoted like $(x, y) \\sim D$ but you will come accross various different notations in the literature. In code, they are the same thing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_batch(xs, ys, B):\n",
        "    \"\"\" Returns a batch of (xs,ys) with B elements \"\"\"\n",
        "    assert len(xs) == len(ys), \"Incompatible number of elements!\"\n",
        "    # YOUR CODE HERE\n",
        "    return xs_batch, ys_batch"
      ],
      "metadata": {
        "id": "5nNwxsFMLMQJ"
      },
      "id": "5nNwxsFMLMQJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1f78456c",
      "metadata": {
        "id": "1f78456c"
      },
      "source": [
        "**2.4.4** *In this step, we will calculate the gradients of the binary cross-entropy loss function with respect to the model parameters $w $ and $ b $ for a given mini-batch of data. These gradients are essential for updating the model parameters during training.*\n",
        "\n",
        "*Write a function `gradient_w, gradient_b = logistic_regression_gradient(xs_batch, ys_batch, w, b)` that calculates the gradients $ \\frac{\\partial L}{\\partial w} $ and $ \\frac{\\partial L}{\\partial b} $ with respect to the batch `xs_batch` (shape $[B, 2]$) and `ys_batch` (shape $[B]$), where $ B $ is the batch size.*\n",
        "\n",
        "\n",
        "*hint: If you are not comfortable with vector operations, we suggest you to first solve this problem using a ```for``` loop. You can then compare your vector-based solution with the iterative ```for``` loop one to make sure you did it right.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"Numerically-stable sigmoid function.\"\n",
        "    return np.exp(-np.logaddexp(0, -z))\n",
        "\n",
        "def logistic_regression_gradient(xs_batch, ys_batch, w, b):\n",
        "    \"\"\" Input: xs_batch [B x 2]\n",
        "               ys_batch [B x 1]\n",
        "        Returns: gradient_w, gradient_b \"\"\"\n",
        "    assert len(xs_batch) == len(ys_batch), \"Incompatible number of elements!\"\n",
        "    assert len(w) == xs_batch.shape[1], \"Incompatible shape of parameter w!\"\n",
        "    # YOUR CODE HERE\n",
        "    return gradient_w, gradient_b # take care that the returned values are calculated for a batch, not a single input/output pair"
      ],
      "metadata": {
        "id": "iK5JCzm8LPkH"
      },
      "id": "iK5JCzm8LPkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fbb4873c",
      "metadata": {
        "id": "fbb4873c"
      },
      "source": [
        "**2.4.5** *Implement and apply Stochastic Gradient Descent (SGD) to solve the logistic regression problem. Start by initializing the model parameters with $w = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $b = 0$. Perform $t = 10^4$ iterations of weight updates using a learning rate of $\\alpha = 10^{-3}$. For each iteration, apply SGD to the dataset `xs` and `ys` using a mini-batch size of $B = 16$. After each update, calculate and record the model's accuracy.*\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = 10**4\n",
        "lr = 10**(-3)\n",
        "w = np.ones(2)\n",
        "ws = np.zeros(shape=[T, 2]) # store the w vector here at each step\n",
        "b = 0.0\n",
        "B = 16\n",
        "accuracy = np.zeros(T)\n",
        "\n",
        "for t in range(T):\n",
        "  # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "XMi6QDN2Ling"
      },
      "id": "XMi6QDN2Ling",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b043f6f4",
      "metadata": {
        "id": "b043f6f4"
      },
      "source": [
        "Let's plot the solution and visualize the decision boundary together with the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa232da",
      "metadata": {
        "id": "efa232da"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "w1, w2 = w\n",
        "yy = np.linspace(np.min(xs[:, 1]), np.max(xs[:, 1]), 20)\n",
        "xx = (w2 * yy + b)/(-w1)\n",
        "plt.figure(1)\n",
        "plt.plot(xx, yy, color=\"orange\")\n",
        "class_membership = np.sign(xs @ w + b)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=class_membership);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9A7QTEp2TgJU",
      "metadata": {
        "id": "9A7QTEp2TgJU"
      },
      "source": [
        "Plot the accuracy as a function of $t$ to see how we converged to a solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BGs2VpuJRAlV",
      "metadata": {
        "id": "BGs2VpuJRAlV"
      },
      "outputs": [],
      "source": [
        "plt.figure(2)\n",
        "plt.plot(accuracy);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f277317c",
      "metadata": {
        "id": "f277317c"
      },
      "source": [
        "## 2.5 Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19a962a5",
      "metadata": {
        "id": "19a962a5"
      },
      "source": [
        "The perceptron model defines a binary classifier of the form $\\hat{y}(x) = \\text{sgn}(\\theta^T x)$, where $x \\in \\mathbb{R}^{d+1}$ is an input vector (with $x_0 = 1$ to include a bias term) and $\\theta \\in \\mathbb{R}^{d+1}$ is a trainable weight vector. The function $\\text{sgn}(\\cdot)$ is the sign function, which maps the input to either $-1$ or $1$, assuming the class labels come from the set $\\{-1, 1\\}$.\n",
        "\n",
        "The basic idea behind the perceptron is to adjust the weight vector $\\theta$ so that $\\theta^T x > 0$ for samples with class label $y = 1$ and $\\theta^T x < 0$ for samples with class label $y = -1$. These criteria can be combined into a single condition:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "y \\cdot \\theta^T x > 0\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81e00613",
      "metadata": {
        "id": "81e00613"
      },
      "source": [
        "In this exercise, we will apply the perceptron model to the same dataset as before. However, we need to adapt the labels to fit the perceptron’s requirements, where $y \\in \\{-1, 1\\}$ instead of $\\{0, 1\\}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563b1577",
      "metadata": {
        "id": "563b1577"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "xs, ys = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, class_sep=.65)\n",
        "ys[ys == 0] = -1  # make the negative class labels == -1\n",
        "plt.figure(0)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IlmTUOottVKF",
      "metadata": {
        "id": "IlmTUOottVKF"
      },
      "source": [
        "Below is a helper function that plots both the dataset and the decision boundary of the perceptron based on the weight vector $\\theta$. We will start by initializing $\\theta$ to $\\theta = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$ and visualizing the resulting decision boundary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LkkAczk2tgr4",
      "metadata": {
        "id": "LkkAczk2tgr4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def plot_perceptron(xs,w,t):\n",
        "  \"\"\" input: xs data\n",
        "             w = parameters [b w1 w2]\n",
        "             t = figure handle\n",
        "  \"\"\"\n",
        "  b, w1, w2 = w\n",
        "  xs_hom = np.concatenate([np.ones(shape=[len(xs), 1]), xs], axis=1)\n",
        "  yy = np.linspace(np.min(xs[:, 1]), np.max(xs[:, 1]), 20)\n",
        "  xx = (w2 * yy + b)/(-w1)\n",
        "  plt.figure(t)\n",
        "  plt.plot(xx, yy, color=\"orange\")\n",
        "  class_membership = np.sign(xs_hom @ w)\n",
        "  plt.scatter(x=xs[:, 0], y=xs[:, 1], c=class_membership)\n",
        "\n",
        "t = plt.figure()\n",
        "w0 = np.ones(2 + 1)\n",
        "plot_perceptron(xs,w0,t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P13XIh8wYMy"
      },
      "source": [
        "**2.5.1** *Implement the perceptron algorithm as described in the Lecture Notes. Your implementation should include an option to plot the decision boundary every $t = 10$ iterations using the provided helper function.*"
      ],
      "id": "7P13XIh8wYMy"
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_algo(xs_hom, w, ys, pl):\n",
        "  \"\"\" input:  xs_hom = [n x d+1] augmented xs\n",
        "              w0 = initial [b w1 w2]\n",
        "              ys = [n x 1] labels\n",
        "      output: w = optimized w\n",
        "              accuracy = list of accuracy over t\n",
        "  \"\"\"\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  if (i % 10 == 0) and (pl):\n",
        "    plot_perceptron(xs,w,i)\n",
        "return w, accuracy, i"
      ],
      "metadata": {
        "id": "a2pyTfLKMFfW"
      },
      "id": "a2pyTfLKMFfW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9zGQfKEwenz"
      },
      "source": [
        "**2.5.2** *Apply the perceptron algorithm to make a single pass over the dataset. Plot the decision boundary every 10 iterations, and also plot the accuracy of the perceptron model over time.*"
      ],
      "id": "y9zGQfKEwenz"
    },
    {
      "cell_type": "code",
      "source": [
        "w0 = np.ones(2 + 1)\n",
        "xs_hom = np.concatenate([np.ones(shape=[len(xs), 1]), xs], axis=1)\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "uXLUnrvKMKSt"
      },
      "id": "uXLUnrvKMKSt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7gmWtNxTA1qh",
      "metadata": {
        "id": "7gmWtNxTA1qh"
      },
      "source": [
        "Recall from the lecture notes the *perceptron loss* which is given by\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "\\ell_{\\text{perceptron}} (y_i, \\hat{y}_i)  &= \\text{max} (0, -y_i \\, \\theta^T x_i)\n",
        "\\end{align}$\n",
        "\n",
        "<br>\n",
        "and can be turned into an average loss\n",
        "\n",
        "$\\begin{align}\n",
        "L(\\theta)  &= \\frac{1}{n} \\sum_{i=1}^n \\, \\text{max} (0, -y_i \\, \\theta^T x_i)\n",
        "\\end{align}$\n",
        "\n",
        "and rewritten\n",
        "\n",
        "$\\begin{align}\n",
        "L(\\theta)  &= - \\frac{1}{n} \\sum_{i \\in \\mathcal{M}} \\, y_i \\, \\theta^T x_i\n",
        "\\end{align}$\n",
        "\n",
        "where $\\mathcal{M}$ are the set of samples that are misclassified $y_i \\theta^T x_i < 0$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2efbd356",
      "metadata": {
        "id": "2efbd356"
      },
      "source": [
        "## 2.6 SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce41c6f",
      "metadata": {
        "id": "cce41c6f"
      },
      "source": [
        "In this exercise, we will explore Support Vector Machines (SVM) using `sklearn.svm`, which is based on the popular `libsvm` library. This implementation uses the dual formulation of the SVM optimization problem and relies on an SMO-like solver to optimize the model parameters $\\theta$. The `sklearn.svm.SVC` model is one of the most widely used tools in machine learning, having made countless predictions across a variety of domains.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's start by generating our dataset, similar to previous exercises, and visualize it to understand the distribution of the classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f741cfc8",
      "metadata": {
        "id": "f741cfc8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "xs, ys = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, class_sep=.65)\n",
        "ys[ys == 0] = -1  # make the negative class labels == -1\n",
        "plt.figure(0)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6008c875",
      "metadata": {
        "id": "6008c875"
      },
      "source": [
        "**2.6.1** *Using `sklearn.svm`, fit an SVM model to the training data and report the model's accuracy on the same dataset.*\n",
        "\n",
        "*Hint*: Training an SVM using `sklearn` is very similar to training a linear regression model. You can refer back to the previous exercises if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"Accuracy: {}%\".format(100.0 * np.mean(ys == ys_pred)))"
      ],
      "metadata": {
        "id": "caRvzE17NxNM"
      },
      "id": "caRvzE17NxNM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "_SDD_ykYzM8g",
      "metadata": {
        "id": "_SDD_ykYzM8g"
      },
      "source": [
        "Below is a plotting function that will help you visualize the decision boundary and margin of your trained SVM model. Use it to plot the results after training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kI1YVkf4zg5q",
      "metadata": {
        "id": "kI1YVkf4zg5q"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "\n",
        "def plot_svm(model, xs, ys, h):\n",
        "  \"\"\"\n",
        "  input:  model: sklearn.svm.SVC model\n",
        "          theta: trained weights\n",
        "          xs:    data\n",
        "          ys:    labels\n",
        "          h:     figure handle\n",
        "  output: plots SVM for 3.9\n",
        "  \"\"\"\n",
        "  # get the separating hyperplane\n",
        "  theta = model.coef_[0]\n",
        "  a = -theta[0] / theta[1]\n",
        "  xx = np.linspace(-1.5, 1)\n",
        "  yy = a * xx - (model.intercept_[0]) / theta[1]\n",
        "  margin = 1 / np.sqrt(np.sum(model.coef_ ** 2))\n",
        "  yy_neg = yy - np.sqrt(1 + a ** 2) * margin\n",
        "  yy_pos = yy + np.sqrt(1 + a ** 2) * margin\n",
        "\n",
        "  # plot the line, the points, and the nearest vectors to the plane\n",
        "  plt.figure(h);\n",
        "  plt.plot(xx, yy, \"k-\")\n",
        "  plt.plot(xx, yy_neg, \"k--\")\n",
        "  plt.plot(xx, yy_pos, \"k--\")\n",
        "  plt.scatter(x=xs[:, 0], y=xs[:, 1], c=ys);\n",
        "  YY, XX = np.meshgrid(yy, xx)\n",
        "  xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "  Z = model.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "  # Put the result into a contour plot\n",
        "  plt.contourf(XX, YY, Z, cmap=cm.get_cmap(\"RdBu\"), alpha=0.5, linestyles=[\"-\"])\n",
        "  plt.xlim([-1.6,1.1])\n",
        "  plt.ylim([-0.24,1.77])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_svm(model,xs,ys,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gISrSVbozibz",
      "metadata": {
        "id": "gISrSVbozibz"
      },
      "source": [
        "The `C` hyperparameter in `sklearn.svm.SVC` controls the trade-off between achieving a larger margin and minimizing classification errors.\n",
        "\n",
        "- A **small `C` value** allows for more slack, leading to a wider margin but potentially more misclassified points.\n",
        "- A **large `C` value** results in a narrower margin, with the model focusing more on classifying every point correctly.\n",
        "\n",
        "Try experimenting with different values of `C` to see how it affects the margin and the classification results.\n",
        "\n",
        "<br>\n",
        "\n",
        "**2.6.2** *Write code to fit two or more SVM models with different `C` values that allow for more slack (i.e., lower `C` values than the default). After fitting the models, analyze the consequences.*\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "cDTnvVyZOMTT"
      },
      "id": "cDTnvVyZOMTT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fbnA-8l4-Q4S",
      "metadata": {
        "id": "fbnA-8l4-Q4S"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sp8xNY8w3B3"
      },
      "source": [
        "## 2.7 Primal-Dual & Constrained Problems"
      ],
      "id": "_sp8xNY8w3B3"
    },
    {
      "cell_type": "markdown",
      "id": "9ee21b8f",
      "metadata": {
        "id": "9ee21b8f"
      },
      "source": [
        "In this exercise, we will solve a constrained optimization problem using primal-dual techniques. Consider an arbitrary point $p = \\begin{bmatrix} p_1 \\\\ p_2 \\\\ p_3 \\end{bmatrix} \\in \\mathbb{R}^3$ and a ball $S$ centered at the point $q = \\begin{bmatrix} q_1 \\\\ q_2 \\\\ q_3 \\end{bmatrix} \\in \\mathbb{R}^3$ with radius $R$. Our goal is to find the point on $S$ that is closest to $p$ in terms of the squared Euclidean distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e951008d",
      "metadata": {
        "id": "e951008d"
      },
      "source": [
        "This *constrained optimization* problem can be phrased as follows:\n",
        "\\begin{align}\n",
        "\\min_{{x}} \\lVert {x} - {p} \\rVert_2^2 \\\\\n",
        "\\text{subject to:} \\lVert {x} - {q} \\rVert_2^2 \\leq R^2\n",
        "\\end{align}\n",
        "\n",
        "which can be rewritten:\n",
        "\\begin{align}\n",
        "\\min_{{x}} f({x})=\\lVert {x} - {p} \\rVert_2^2 \\\\\n",
        "\\text{subject to: } g({x})= \\lVert {x} - {q} \\rVert_2^2 - R^2 \\leq 0\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "The Lagrangian primal problem is:\n",
        "\\begin{align}\n",
        "\\min_{x \\in \\mathbb{R}} \\mathcal{\\psi_P}(x)  \\\\\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "and the Lagrangian dual problem is then given by:\n",
        "\\begin{align}\n",
        "\\max_{\\alpha \\in \\mathbb{R}} \\mathcal{\\psi_D}(\\alpha)  \\\\\n",
        "\\text{subject to: } \\alpha \\geq 0\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "where $\\psi_\\mathcal{D}(\\alpha) = \\min_{{x}} \\mathcal{L}({x}, \\alpha)$ and $\\psi_\\mathcal{P}(\\alpha) = \\max_{{\\alpha}} \\mathcal{L}({x}, \\alpha)$, and $\\mathcal{L}({x}, \\alpha) = f({x}) + \\alpha g({x})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmmIuFPrw8an"
      },
      "source": [
        "**2.7.1** *Give the expression for the Lagrangian $\\mathcal{L}$.*"
      ],
      "id": "bmmIuFPrw8an"
    },
    {
      "cell_type": "markdown",
      "id": "q47omjNDdlwI",
      "metadata": {
        "id": "q47omjNDdlwI"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XAVytTuxAXW"
      },
      "source": [
        "**2.7.2** *Derive the gradient $\\frac{\\partial L}{\\partial x}$ and set it equal to zero $\\frac{\\partial L}{\\partial x} = 0$. Simplify the expression to obtain the condition that must be satisfied at the optimal point.*"
      ],
      "id": "6XAVytTuxAXW"
    },
    {
      "cell_type": "markdown",
      "id": "e36d0c56",
      "metadata": {
        "id": "e36d0c56"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3XPAewjxPA8"
      },
      "source": [
        "## 2.8 K-Medoids Clustering\n",
        "\n"
      ],
      "id": "I3XPAewjxPA8"
    },
    {
      "cell_type": "markdown",
      "id": "cc7ef85a",
      "metadata": {
        "id": "cc7ef85a"
      },
      "source": [
        "Assume we have a dataset $\\mathcal{D} = \\lbrace \\mathbf{x}_i \\rbrace_{i=1}^N$ *without* any labels. Our goal is to identify a specific number of clusters, denoted by $K$, where data points tend to \"gather\" around certain central points, called medoids.\n",
        "\n",
        "One effective method for clustering is **K-Medoids**. Unlike K-Means, which uses the mean of points in a cluster, K-Medoids selects actual data points as cluster centers (medoids). This method requires specifying $K$ in advance, reflecting our prior knowledge of how many clusters we expect in the data.\n",
        "\n",
        "We begin by initializing the cluster centers, $\\mathbf{S}_k$ ($k=1,\\ldots,K$), by randomly selecting $K$ points from the dataset without repetition. These points serve as the initial medoids around which the other data points will be clustered.\n",
        "\n",
        "To cluster the data points around the medoids, we use an iterative method known as *Voronoi iteration* or *Lloyd's Algorithm*. This algorithm involves two main steps:\n",
        "\n",
        "1. **Assign**: Map each point $\\mathbf{x}_i \\in \\mathcal{D}$ to the closest medoid $\\mathbf{S}_k$ based on a chosen distance metric (Euclidean distance in this case).\n",
        "2. **Update**: For each cluster $\\mathbf{S}_k$, recompute the medoid by selecting the point within the cluster that minimizes the total distance to all other points in that cluster. This point is called the medoid.\n",
        "3. **Repeat** step 1.\n",
        "\n",
        "\n",
        "Your task is to implement the following three functions:\n",
        "1. `initialize_cluster_centres_from_random_points`: Randomly select $K$ initial medoids from the dataset.\n",
        "2. `assign`: Assign each point to the nearest medoid.\n",
        "3. `update`: Recalculate the medoids based on the current assignments.\n",
        "\n",
        "*Hint*: You can use the `scipy.spatial.distance.cdist` function to compute pairwise distances between points efficiently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3afab276",
      "metadata": {
        "id": "3afab276"
      },
      "source": [
        "**2.8.1** *Implement the function to initialize $K$ cluster centers `initialize_cluster_centres_from_random_points(xs, K)`. This function should randomly select $K$ distinct points from the dataset `xs` to serve as the initial medoids.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_cluster_centres_from_random_points(xs, K):\n",
        "    \"\"\"\n",
        "    Randomly samples K points from 'xs'\n",
        "    Returns\n",
        "      An np.ndarray of shape [K, d] containing the initial medoids\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "xr7mA8y2YeWt"
      },
      "id": "xr7mA8y2YeWt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "810ed400",
      "metadata": {
        "id": "810ed400"
      },
      "source": [
        "**2.8.2** *Implement the function `assign(xs, m)` to assign each data point $x_i$ in `xs` to the nearest medoid in `m`. The function should return a vector of cluster indices $z_i \\in \\{1,\\ldots,K\\}$ corresponding to the closest medoid for each point.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def assign(xs, m):\n",
        "    \"\"\"\n",
        "    Computes a vector of cluster indices for each point in 'xs'. Each point gets assigned to the closest cluster.\n",
        "      xs: An np.ndarray of shape [N, d] where d is the dimension of the points\n",
        "      m: An np.ndarray of shape [K, d] where K is the number of clusters\n",
        "\n",
        "    Returns:\n",
        "      An np.ndarray of shape [N]\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return z"
      ],
      "metadata": {
        "id": "__pUfMMMYkbg"
      },
      "id": "__pUfMMMYkbg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "358d1c63",
      "metadata": {
        "id": "358d1c63"
      },
      "source": [
        "**2.8.3** *Implement the function `medoid_point(pts)` to return the medoid $m_k$ of a given set of points $S_k$. This function should calculate which point within $S_k$ minimizes the total distance to all other points in $S_k$ and return that point as the medoid.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def medoid_point(pts):\n",
        "    \"\"\"\n",
        "    Computes the medoid point (w.r.t. the Euclidean distance)\n",
        "    pts: A np.ndarray of shape [N, d]\n",
        "\n",
        "    Returns:\n",
        "      An np.ndarray of shape [d] (the medoid)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "2_JNG1m3YwAN"
      },
      "id": "2_JNG1m3YwAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0513336b",
      "metadata": {
        "id": "0513336b"
      },
      "source": [
        "**2.8.4** *Implement the function `update(xs, m, z)` to assign new medoids that minimize the total distance to members within each cluster. For each cluster, use the function `medoid_point` to find the new medoid and update the array `m`.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update(xs, m, z):\n",
        "    \"\"\"\n",
        "        xs: An np.ndarray of shape [N, d] where d is the dimension of the points\n",
        "        m: An np.ndarray of shape [K, d] where K is the number of clusters\n",
        "        z: An np.ndarray of shape [N] that maps each point to a cluster index\n",
        "\n",
        "    Returns:\n",
        "       An np.ndarray of shape [K, d]\n",
        "    \"\"\"\n",
        "    K, d = m.shape\n",
        "    new_m = np.zeros_like(m)\n",
        "    for k in range(K):\n",
        "      # YOUR CODE HERE\n",
        "    return new_m"
      ],
      "metadata": {
        "id": "SBBbiaNsY389"
      },
      "id": "SBBbiaNsY389",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "27989a67",
      "metadata": {
        "id": "27989a67"
      },
      "source": [
        "Now, let's apply the functions on our favorite dataset. First, plot the unassigned points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0124c1b2",
      "metadata": {
        "id": "0124c1b2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "xs,_ = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, class_sep=.65)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1v3res8pOhCt",
      "metadata": {
        "id": "1v3res8pOhCt"
      },
      "source": [
        "Now, pick some initial medoids. We will use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47da3899",
      "metadata": {
        "id": "47da3899"
      },
      "outputs": [],
      "source": [
        "K = 2\n",
        "m = initialize_cluster_centres_from_random_points(xs, K)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1])\n",
        "plt.scatter(x=m[:, 0], y=m[:, 1], marker=\"^\", linewidth=5);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6cdd183",
      "metadata": {
        "id": "b6cdd183"
      },
      "source": [
        "Apply the assignment step to get an initial clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "neaP-UWBOfx-",
      "metadata": {
        "id": "neaP-UWBOfx-"
      },
      "outputs": [],
      "source": [
        "z = assign(xs, m)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=z)\n",
        "plt.scatter(x=m[:, 0], y=m[:, 1], marker=\"^\", linewidth=5);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218EYKzxPlAE",
      "metadata": {
        "id": "218EYKzxPlAE"
      },
      "source": [
        "**2.8.5** *Repeat the `update` and `assign` steps $t$ times, where $t$ is the number of iterations. After each iteration, plot the current clustering results to visualize how the clusters and medoids evolve over time.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = 3\n",
        "for i in range(t):\n",
        "  # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "swUgxIhHZAUc"
      },
      "id": "swUgxIhHZAUc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}